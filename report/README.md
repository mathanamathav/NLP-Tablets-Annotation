# NLP Tablets Annotation

---

> To extract necessary details such as the name of the medicine, its molecules, date of manufacturing, and date of expiry from a set of tablet images, OCR can be used to convert the image to text. This text can then be converted into speech using an API for text to speech conversion. Creating a drug database and forming a lexicon can be helpful for this process, and drug details can be scraped to populate the database.
> 

Let’s divide the given project into set of three steps and discuss the each steps in details , compare various techniques that can be also be used which may improve our project.

![CPT2303221823-1858x885.gif](NLP%20Tablets%20Annotation/CPT2303221823-1858x885.gif)

# `1`  OCR - Comparing Pytesseract and Paddle OCR

---

Optical Character Recognition (OCR) is a technology that enables the conversion of scanned images and handwritten or printed text into machine-encoded text. OCR has diverse applications, including document digitization, image indexing, and automated data entry. In this blog post, we will compare two popular OCR libraries, Pytesseract and Paddle OCR, and explore their strengths and weaknesses.

![https://www.docsvault.com/wordpress/wp-content/uploads/2022/07/Simple-OCR.gif](https://www.docsvault.com/wordpress/wp-content/uploads/2022/07/Simple-OCR.gif)

## ➡️ Pytesseract

> Pytesseract is an open-source OCR engine developed by Google. It is based on the Tesseract OCR engine, which was originally developed by HP Labs and later maintained by Google. Tesseract is widely used in various industries, including healthcare, finance, and legal. Pytesseract is easy to install and use, and it supports multiple languages.
> 
> 
> Pytesseract is known for its accuracy and speed. It can recognize text from various image formats, including PNG, JPEG, and TIFF. The library also allows users to configure the OCR engine by adjusting various parameters, such as page segmentation mode, OCR engine mode, and whitelist/blacklist characters.
> 
> However, Pytesseract has its limitations. It struggles with low-quality images, distorted text, and non-standard fonts. The library also lacks advanced features, such as text recognition in multiple orientations and languages, which can be a drawback for some users.
> 

## ➡️Paddle OCR

> Paddle OCR is an OCR toolkit developed by PaddlePaddle, an open-source deep learning platform. Paddle OCR is based on deep learning algorithms, which enable it to recognize text with high accuracy and speed. The toolkit can recognize text in various languages and scripts, including Chinese, Japanese, and Korean.
> 
> 
> Paddle OCR offers advanced features, such as text recognition in multiple orientations and languages, text detection, and character segmentation. The library also includes pre-trained models for document recognition, scene text recognition, and handwritten text recognition.
> 
> Paddle OCR's deep learning algorithms make it more robust than traditional OCR engines, especially when dealing with low-quality images, distorted text, and non-standard fonts. However, Paddle OCR can be challenging to install and configure, and it may require some knowledge of deep learning concepts.
> 

# Paddle OCR Deep Learning Architecture Explained

Paddle OCR is based on a deep learning architecture that combines convolutional neural networks (CNNs), recurrent neural networks (RNNs), and attention mechanisms. The architecture is designed to recognize text in various languages and scripts, including Chinese, Japanese, and Korean.

### Convolutional Neural Networks (CNNs)

Convolutional Neural Networks (CNNs) are a type of neural network that is commonly used in image recognition tasks. CNNs consist of multiple layers of filters that learn to recognize low-level features in images, such as edges, corners, and textures. The filters are then combined to recognize higher-level features, such as shapes and objects.

In Paddle OCR, CNNs are used to extract features from input images. The input image is passed through multiple convolutional layers, each of which applies a set of filters to the input image. The output of the convolutional layers is a set of feature maps that capture the important features of the input image.

### Recurrent Neural Networks (RNNs)

Recurrent Neural Networks (RNNs) are a type of neural network that is commonly used in sequential data analysis tasks. RNNs consist of a set of interconnected nodes that process input sequences one element at a time. Each node maintains a memory state that allows it to capture information from previous elements in the sequence.

In Paddle OCR, RNNs are used to decode the output of the CNNs. The feature maps generated by the CNNs are passed through a set of recurrent layers, each of which processes one element of the feature maps. The output of the recurrent layers is a sequence of vectors that represents the probability of each character in the output text.

### Attention Mechanisms

Attention mechanisms are a type of neural network that is commonly used in sequence-to-sequence tasks. Attention mechanisms allow the model to focus on the most relevant parts of the input sequence when generating the output sequence.

In Paddle OCR, attention mechanisms are used to improve the accuracy of the model by allowing it to focus on the most relevant parts of the input image. The attention mechanism takes the output of the CNNs and the output of the RNNs and generates a set of attention weights that indicate which parts of the input image are most relevant for generating the output text.

### CTC Loss

Connectionist Temporal Classification (CTC) Loss is a loss function commonly used in OCR tasks. CTC loss allows the model to learn to recognize sequences of characters without the need for explicit alignment between the input image and the output text.

In Paddle OCR, CTC loss is used to train the model to recognize sequences of characters in the input image. The CTC loss function takes the output of the attention mechanism and the ground truth text and computes the difference between them.

# Comparison Pytesseract vs Paddle OCR: A Closer Look

Pytesseract and Paddle OCR have some similarities and differences that are worth exploring in more detail. Let's take a closer look at the two OCR engines and compare them in terms of their features, performance, and ease of use.

![Sample preprocessed medicine image with paddle OCR with  `783`  characters ](NLP%20Tablets%20Annotation/paddle.png)

Sample preprocessed medicine image with paddle OCR with  `783`  characters 

![Sample preprocessed medicine image with pytesseract with `523` characters ](NLP%20Tablets%20Annotation/pytesseract.png)

Sample preprocessed medicine image with pytesseract with `523` characters 

### Features

One of the main differences between Pytesseract and Paddle OCR is the features they offer. Pytesseract is a simple OCR engine that can recognize text from various image formats and supports multiple languages. It also allows users to configure the OCR engine by adjusting various parameters, such as **page segmentation mode**, **OCR engine mode**, and **whitelist/blacklist characters**.

Paddle OCR, on the other hand, is a more advanced OCR toolkit that offers a wide range of features, including text recognition in **multiple orientations** and **languages, text detection**, and **character segmentation**. The library also includes pre-trained models for document recognition, scene text recognition, and handwritten text recognition. Paddle OCR's deep learning algorithms make it more robust than traditional OCR engines, especially when dealing with low-quality images, distorted text, and non-standard fonts.

### Ease of Use

Pytesseract and Paddle OCR also differ in terms of ease of use. Pytesseract is easy to install and use, and it supports multiple programming languages. The library also allows users to configure the OCR engine by adjusting various parameters, such as page segmentation mode, OCR engine mode, and whitelist/blacklist characters. Paddle OCR, on the other hand, can be challenging to install and configure, and it may require some knowledge of deep learning concepts.

Both Pytesseract and Paddle OCR are powerful OCR engines with their own strengths and weaknesses. Pytesseract is a good choice for users who need a simple and easy-to-use OCR engine that supports multiple languages. Paddle OCR is a good choice for users who need advanced features and high accuracy, especially when dealing with challenging images. Ultimately, the choice between Pytesseract and Paddle OCR depends on the specific use case and the user's expertise.

# `2`  Custom Named Entity Recognition using Spacy

---

Named Entity Recognition (NER) is a natural language processing task that involves identifying and classifying named entities in text into predefined categories such as person, organization, or location. Spacy is a powerful open-source library in Python that provides an easy-to-use API for NER. In this blog post, we'll explore how to train a custom NER model using Spacy.

## Introduction to Spacy

Spacy is a Python library that is designed to help developers build natural language processing applications. It provides a simple and efficient way to perform tasks such as tokenization, part-of-speech tagging, and named entity recognition. Spacy also provides pre-trained models for these tasks, which can be easily customized to suit specific use cases.

## Creating a Custom NER Model

To create a custom NER model using Spacy, we need to follow these steps:

1. **Gather and annotate training data:** The first step is to gather a dataset of text that includes the named entities we want to identify. We then need to annotate this data to mark up the named entities with the correct labels.
2. **Split data into training and testing sets:** Once we have annotated the data, we need to split it into training and testing sets. This is important to evaluate our model's performance.
3. **Train a model:** We can train a custom NER model using Spacy's `train` function. The function takes the annotated data as input and learns to recognize the named entities we have labeled.
4. **Evaluate the model:** Once we have trained the model, we need to evaluate it using the testing data. We can use Spacy's `evaluate` function to measure the model's accuracy.
5. **Save the model:** Finally, we can save the trained model so that we can use it in our applications.

## Annotating the Data

To annotate the data, we need to mark up the named entities with the correct labels. Spacy uses the IOB format (Inside, Outside, Beginning) to annotate the named entities. For example, if we want to identify the name of a person, we would annotate the text as follows:

```
John Smith is a software engineer at Google.
B-PER I-PER  O      O       O    O   B-ORG

```

The `B-PER` and `I-PER` tags indicate the beginning and continuation of the named entity, respectively. The `O` tag indicates that the word is not part of a named entity.

## Training the Model

To train the model, we need to use Spacy's `train` function. The function takes the annotated data as input and learns to recognize the named entities we have labeled. We also need to specify the number of iterations and the dropout rate. The dropout rate is used to prevent overfitting.

```
import spacy
from spacy.training.example import Example

nlp = spacy.blank("en")
ner = nlp.add_pipe("ner")

# Add the labels we want to recognize
ner.add_label("PERSON")

# Train the model
for itn in range(10):
    random.shuffle(TRAIN_DATA)
    losses = {}
    for text, annotations in TRAIN_DATA:
        doc = nlp.make_doc(text)
        example = Example.from_dict(doc, annotations)
        nlp.update([example], losses=losses, drop=0.5)
    print(losses)

```

## Evaluating the Model

To evaluate the model, we can use Spacy's `evaluate` function. The function takes the testing data as input and measures the model's accuracy. We can also calculate the precision, recall, and F1 score of the model.

```
# Evaluate the model
scores = nlp.evaluate(TEST_DATA)
print(scores)

```

## Using the Model

Once we have trained and evaluated the model, we can use it to identify named entities in new text. We can use the `doc.ents` attribute to get a list of named entities in the document.

```
# Use the model
doc = nlp("John Smith is a software engineer at Google.")
for ent in doc.ents:
    print(ent.text, ent.label_)

```

# `3`  Text to Speech model used

---

# Google's Text to Speech (TTS) Model explained

Text to Speech (TTS) technology is a type of assistive technology that helps people with visual impairment, reading difficulties, or learning disabilities to access digital content. TTS technology converts written **text into spoken words**, enabling users to listen to the text instead of reading it. Google's TTS model is one of the most popular TTS models available. 

## Features

Google's TTS model is based on **deep learning algorithms**, which enable it to generate natural-sounding speech. The model can speak in multiple languages, including **English, Spanish, French, German, and Japanese**. It also offers a wide range of voices, including male and female voices, adult and child voices, and accents from different regions.

The TTS model also allows users to customize the **speed, pitch, and volume** of the speech. Users can adjust the settings to match their preferences or to make the speech more understandable for people with hearing impairments.

Google's TTS model is available as an **API**, which means that developers can integrate it into their applications or websites. The API is easy to use and offers a wide range of options, including text input, voice selection, and audio output format.

# Understanding the Architecture of Text-to-Speech (TTS) Models

Text-to-Speech (TTS) technology has come a long way in recent years. With the advancements in deep learning, TTS models have become more accurate and natural-sounding, making them an indispensable tool for people with visual impairments or reading difficulties. In this blog post, we will dive into the architecture of TTS models and explore how they work.

## The Components of a TTS Model

A TTS model consists of three main components: the **text analysis module, the acoustic model, and the audio synthesis module**. Let's take a closer look at each of these components.

### Text Analysis Module

The text analysis module takes the input text and converts it into a sequence of linguistic features, such as **phonemes, syllables, and prosody**. This module is responsible for breaking down the text into its constituent parts and extracting the **linguistic information** required for generating speech.

### Acoustic Model

The acoustic model takes the linguistic features generated by the text analysis module and converts them into a sequence of acoustic features, such as spectral features and pitch contours. This module is responsible for modeling the **relationship between linguistic features and acoustic features** and generating a set of **acoustic parameters** that can be used to synthesize speech.

### Audio Synthesis Module

The audio synthesis module takes the acoustic features generated by the acoustic model and converts them into an **audio waveform**. This module is responsible for synthesizing speech from the acoustic parameters generated by the acoustic model and producing a natural-sounding audio output.

## Deep Learning in TTS Models

Deep learning has revolutionized the field of TTS by enabling the creation of more accurate and natural-sounding models. In particular, **deep neural networks** (DNNs) have been used to model the complex relationships between linguistic features and acoustic features.

### WaveNet

WaveNet is a deep neural network architecture developed by Google for TTS. WaveNet is based on a **generative model** that learns to directly model the audio waveform of speech. The model is trained using a variant of the **generative adversarial network** (GAN) algorithm, which enables it to generate high-quality audio samples that sound natural and realistic.

WaveNet has been shown to be highly effective at synthesizing speech, but it is also computationally expensive and requires a large amount of training data. As a result, WaveNet is not always practical for real-time applications.

### Tacotron

Tacotron is a deep neural network architecture developed by Google for TTS. Tacotron is based on an encoder-decoder architecture that learns to map input text to a sequence of acoustic features. The model is trained using a variant of the attention mechanism, which enables it to focus on the most relevant parts of the input text when generating the acoustic features.

Tacotron has been shown to be highly effective at synthesizing speech and is more computationally efficient than WaveNet. However, Tacotron requires a large amount of training data and may still struggle with certain types of speech, such as speech with strong accents or speech in noisy environments.

### Transformer TTS

Transformer TTS is a deep neural network architecture developed by Microsoft for TTS. Transformer TTS is based on the transformer architecture, which was originally developed for natural language processing tasks. The model is trained using a variant of the attention mechanism, which enables it to focus on the most relevant parts of the input text when generating the acoustic features.

Transformer TTS has been shown to be highly effective at synthesizing speech and is more computationally efficient than both WaveNet and Tacotron. Transformer TTS also requires less training data than Tacotron, making it a **more practical choice** for real-world applications.

# 4️⃣ Improvising our projects

With a deep understanding of various models used in the project, there are a few things we can do to improve our project even further:

1. Use a better OCR model that captures more text details, even under bad lighting conditions. Alternatively, build a custom OCR model trained using medical covers to capture finer details.
2. Introduce various image processing techniques to capture the tablet cover image without any loss of information.
3. Improve the custom named entity recognition model with a larger dataset. Since the scope of our project is small, we limited the model to train on 60 images. We can hyper-tune the parameters to improve its performance.

# Summary

This document discusses the use of OCR, Spacy for custom named entity recognition, and Google's Text-to-Speech model for extracting necessary details from a set of tablet images. The document compares Pytesseract and Paddle OCR for OCR, explains the architecture of TTS models, and suggests ways to improve the project, such as using better OCR models and improving the custom named entity recognition model.

# Team

This project was only made possible by the tireless contributions and collaborative efforts of each and every member of the team. By working together and sharing their unique perspectives, they were able to overcome any obstacles that arose during the development process. By doing so, they ensured that the final product was of the highest quality, meeting and exceeding all expectations. The team's dedication and hard work truly set them apart, and they should be proud of their accomplishments.

**A.S MATHAN MATHAV** - *19PD01*

**JEGADEESH MANICKAM M S** - *19PD15*

**PARTHASARATHY S** - *19PD25*

![https://media0.giphy.com/media/3o6ozuHcxTtVWJJn32/200w.gif?cid=82a1493bg13szqwr9pfmi86qt7adx3ca044at92bi51ojkh7&rid=200w.gif&ct=g](https://media0.giphy.com/media/3o6ozuHcxTtVWJJn32/200w.gif?cid=82a1493bg13szqwr9pfmi86qt7adx3ca044at92bi51ojkh7&rid=200w.gif&ct=g)